{"cells":[{"cell_type":"markdown","metadata":{"id":"o8ewEvUkSRx4"},"source":["Install necessary packages"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"AhPrFHgCy4-f","executionInfo":{"status":"ok","timestamp":1696674476422,"user_tz":-330,"elapsed":13318,"user":{"displayName":"Srikanth Shenoy","userId":"02394031778717342336"}}},"outputs":[],"source":["!pip install -q transformers einops accelerate langchain bitsandbytes"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lokhz1dbzexR","outputId":"65a78931-54c3-476a-fa2a-527e2e59357c","executionInfo":{"status":"ok","timestamp":1696673099015,"user_tz":-330,"elapsed":4978,"user":{"displayName":"Srikanth Shenoy","userId":"02394031778717342336"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (0.17.3)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub) (3.12.4)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub) (2023.6.0)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub) (2.31.0)\n","Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub) (4.66.1)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub) (6.0.1)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub) (4.5.0)\n","Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub) (23.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub) (3.3.0)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub) (3.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub) (2.0.6)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub) (2023.7.22)\n"]}],"source":["!pip install huggingface-hub\n","\n","#!huggingface-cli login"]},{"cell_type":"markdown","metadata":{"id":"Ck8n414nSm_c"},"source":["Follow below steps to generate access tokens from Hugging face website\n","\n","1.   Open https://huggingface.co/ and sign up with your enail\n","2.   Verify email ID from your inbox\n","3.  Post that, from the Hugging face page open settings from top right corner\n","4. In the settings, go to Access tokens -> New token -> give a name and select READ access\n","5. Copy the token and paste in below cell\n","\n"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"FrgiaMD4zk3v","executionInfo":{"status":"ok","timestamp":1696674539925,"user_tz":-330,"elapsed":8,"user":{"displayName":"Srikanth Shenoy","userId":"02394031778717342336"}}},"outputs":[],"source":["access_token = \"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\" #\"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\""]},{"cell_type":"markdown","metadata":{"id":"peb3qRIuZvjz"},"source":["Let us login to HuggingFace now with the access key available"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ParPk1Bd_OPU","outputId":"cad6088b-8292-4608-dc3d-ee503daa666b","executionInfo":{"status":"ok","timestamp":1696674542996,"user_tz":-330,"elapsed":3077,"user":{"displayName":"Srikanth Shenoy","userId":"02394031778717342336"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Token will not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n","Token is valid (permission: read).\n","Your token has been saved to /root/.cache/huggingface/token\n","Login successful\n"]}],"source":["from huggingface_hub import login\n","access_token_read = access_token\n","login(token = access_token_read)"]},{"cell_type":"markdown","metadata":{"id":"mX7IbtRoT8Vl"},"source":["Summary of below code:\n","\n","*   Here we are specifying the path to the Llama 2 7B version in Hugging Face to the model variable\n","*   Then we download the tokenizer for the Llama 2 7B model by specifying the model name to the AutoTokenizer\n","*   We use the transformer pipeline function and pass all the parameters to it.\n","*   The device_map = auto tokenizer will allow the model to use the GPU in colab if present.. For more on the pipeline parameters refer: https://huggingface.co/docs/transformers/main_classes/pipelines#transformers.pipeline.device\n","\n","\n","\n","\n","\n"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"PxfFcQFjzz3r","outputId":"2d597c1d-2e88-47ac-f029-71a593ef0ab4","executionInfo":{"status":"error","timestamp":1696674548322,"user_tz":-330,"elapsed":3001,"user":{"displayName":"Srikanth Shenoy","userId":"02394031778717342336"}}},"outputs":[{"output_type":"stream","name":"stderr","text":["The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n"]},{"output_type":"error","ename":"ValueError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-5-3e56939fea28>\u001b[0m in \u001b[0;36m<cell line: 11>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoTokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m pipeline = transformers.pipeline(\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0;34m\"text-generation\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/pipelines/__init__.py\u001b[0m in \u001b[0;36mpipeline\u001b[0;34m(task, model, config, tokenizer, feature_extractor, image_processor, framework, revision, use_fast, token, device, device_map, torch_dtype, trust_remote_code, model_kwargs, pipeline_class, **kwargs)\u001b[0m\n\u001b[1;32m    832\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mframework\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    833\u001b[0m         \u001b[0mmodel_classes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"tf\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtargeted_task\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"tf\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"pt\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtargeted_task\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"pt\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 834\u001b[0;31m         framework, model = infer_framework_load_model(\n\u001b[0m\u001b[1;32m    835\u001b[0m             \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    836\u001b[0m             \u001b[0mmodel_classes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel_classes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py\u001b[0m in \u001b[0;36minfer_framework_load_model\u001b[0;34m(model, config, model_classes, task, framework, **model_kwargs)\u001b[0m\n\u001b[1;32m    280\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mclass_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrace\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mall_traceback\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    281\u001b[0m                 \u001b[0merror\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34mf\"while loading with {class_name}, an error is thrown:\\n{trace}\\n\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 282\u001b[0;31m             raise ValueError(\n\u001b[0m\u001b[1;32m    283\u001b[0m                 \u001b[0;34mf\"Could not load model {model} with any of the following classes: {class_tuple}. See the original errors:\\n\\n{error}\\n\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    284\u001b[0m             )\n","\u001b[0;31mValueError\u001b[0m: Could not load model daryl149/llama-2-7b-chat-hf with any of the following classes: (<class 'transformers.models.auto.modeling_auto.AutoModelForCausalLM'>, <class 'transformers.models.auto.modeling_tf_auto.TFAutoModelForCausalLM'>, <class 'transformers.models.llama.modeling_llama.LlamaForCausalLM'>). See the original errors:\n\nwhile loading with AutoModelForCausalLM, an error is thrown:\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py\", line 269, in infer_framework_load_model\n    model = model_class.from_pretrained(model, **kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/auto/auto_factory.py\", line 565, in from_pretrained\n    return model_class.from_pretrained(\n  File \"/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py\", line 3307, in from_pretrained\n    ) = cls._load_pretrained_model(\n  File \"/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py\", line 3428, in _load_pretrained_model\n    raise ValueError(\nValueError: The current `device_map` had weights offloaded to the disk. Please provide an `offload_folder` for them. Alternatively, make sure you have `safetensors` installed if the model you are using offers the weights in this format.\n\nwhile loading with TFAutoModelForCausalLM, an error is thrown:\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py\", line 269, in infer_framework_load_model\n    model = model_class.from_pretrained(model, **kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/auto/auto_factory.py\", line 568, in from_pretrained\n    raise ValueError(\nValueError: Unrecognized configuration class <class 'transformers.models.llama.configuration_llama.LlamaConfig'> for this kind of AutoModel: TFAutoModelForCausalLM.\nModel type should be one of BertConfig, CamembertConfig, CTRLConfig, GPT2Config, GPT2Config, GPTJConfig, OpenAIGPTConfig, OPTConfig, RemBertConfig, RobertaConfig, RobertaPreLayerNormConfig, RoFormerConfig, TransfoXLConfig, XGLMConfig, XLMConfig, XLMRobertaConfig, XLNetConfig.\n\nwhile loading with LlamaForCausalLM, an error is thrown:\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py\", line 269, in infer_framework_load_model\n    model = model_class.from_pretrained(model, **kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py\", line 3307, in from_pretrained\n    ) = cls._load_pretrained_model(\n  File \"/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py\", line 3428, in _load_pretrained_model\n    raise ValueError(\nValueError: The current `device_map` had weights offloaded to the disk. Please provide an `offload_folder` for them. Alternatively, make sure you have `safetensors` installed if the model you are using offers the weights in this format.\n\n\n"]}],"source":["\n","from langchain import HuggingFacePipeline\n","from transformers import AutoTokenizer\n","import transformers\n","import torch\n","\n","#\"meta-llama/Llama-2-7b-chat-hf\"\n","model = \"daryl149/llama-2-7b-chat-hf\"\n","\n","tokenizer = AutoTokenizer.from_pretrained(model)\n","\n","pipeline = transformers.pipeline(\n","    \"text-generation\",\n","    model=model,\n","    tokenizer=tokenizer,\n","    torch_dtype=torch.bfloat16,\n","    trust_remote_code=True,\n","    device_map=\"auto\",\n","    max_length=1000,\n","    eos_token_id=tokenizer.eos_token_id\n",")"]},{"cell_type":"markdown","metadata":{"id":"PgUBiLL2U474"},"source":["Here we set the model’s temperature and pass the pipeline we created to the pipeline variable. This HuggingFacePipeline will now allow us to use the model that we have downloaded."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JEjouNTwDnlC"},"outputs":[],"source":["llm = HuggingFacePipeline(pipeline = pipeline, model_kwargs = {'temperature':0})"]},{"cell_type":"markdown","metadata":{"id":"DDkG4Wp8VDry"},"source":["*   Template: We want the Llama model to answer the user’s query and return it as points with numbering.\n","*   Then we pass this template to the PrompTemplate function and assign the template and the input_variable parameters.\n","*   Finally, we chain our Llama LLM and the Prompt to start inferencing the model.\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JR7ioX-1E0QD"},"outputs":[],"source":["from langchain import PromptTemplate,  LLMChain\n","\n","template = \"\"\"\n","              You are an intelligent chatbot that gives out useful information to humans.\n","              You return the responses in sentences with arrows at the start of each sentence\n","              {query}\n","           \"\"\"\n","\n","prompt = PromptTemplate(template=template, input_variables=[\"query\"])\n","\n","llm_chain = LLMChain(prompt=prompt, llm=llm)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QdXajThqiMB6","outputId":"adc8684b-8670-4b96-d951-fc35b23ccfa2"},"outputs":[{"name":"stdout","output_type":"stream","text":[" ^\n","\n","    ```\n","    Sure, I'd be happy to help! Global warming is the gradual increase in the overall temperature of the Earth's atmosphere, primarily caused by human activities that release large amounts of greenhouse gases, such as carbon dioxide and methane, into the atmosphere. These gases trap heat from the sun, causing the Earth's temperature to rise. The effects of global warming include more frequent and severe heatwaves, droughts, and storms, as well as rising sea levels and more acidic oceans. It's important for us to reduce our greenhouse gas emissions in order to slow down the rate of global warming and prevent its worst impacts.\n","    ^\n","```\n","\n"]}],"source":["print(llm_chain.run('Can you tell me about Global Warming?'))"]},{"cell_type":"markdown","metadata":{"id":"wkaA6mNTZQa3"},"source":["NOTE: This is a very basic and entry level open source model by META and can answer basic questions to some accuracy."]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.12"}},"nbformat":4,"nbformat_minor":0}